import numpy as np
import logging

import igp2 as ip
from typing import Dict, List
from dataclasses import dataclass
from sklearn.linear_model import LogisticRegression

from xavi.features import Features
from xavi.util import fill_missing_actions
from xavi.matching import matching

logger = logging.getLogger(__name__)


@dataclass
class Item:
    """ Class to store a (counter)factual trajectories, the actions of the ego,
    the rewards, and the query observation generated by MCTS. """
    trajectories: Dict[int, ip.StateTrajectory]
    query_present: bool
    reward: Dict[str, float]


class XAVIAgent(ip.MCTSAgent):
    """ Generate new rollouts and save results after MCTS in the observation tau time before. """

    def __init__(self, tau: int = None,
                 cf_n_trajectories: int = 3,
                 cf_n_simulations: int = 15,
                 cf_max_depth: int = 5,
                 user_query: dict = None,
                 **kwargs):
        """ Create a new XAVIAgent.

        Args:
            tau: The interval to roll back for counterfactual generation. By default set to FPS.
            cf_n_trajectories: Number of maximum trajectories to generate with A*.
            cf_n_simulations: Number of MCTS simulations to run for counterfactual generation.
            cf_d_max: Maximum MCTS search depth for counterfactual simulations.

        Keyword Args: See arguments of parent-class MCTSAgent.
        """

        super(XAVIAgent, self).__init__(**kwargs)

        self.__n_trajectories = cf_n_trajectories
        self.__previous_goal_probabilities = None
        self.__previous_mcts = ip.MCTS(scenario_map=kwargs["scenario_map"],
                                       n_simulations=kwargs.get("cf_n_simulations", cf_n_simulations),
                                       max_depth=kwargs.get("cf_max_depth", cf_max_depth),
                                       store_results="all")
        self.__features = Features()
        self.__scenario_map = kwargs["scenario_map"]
        self.__tau = tau if tau is not None else kwargs["fps"]
        assert self.__tau >= 0, f"Tau cannot be negative. "

        self.__previous_observations = None
        self.__dataset = None
        self.__user_query = user_query
        self.__matching = matching()

    def explain_actions(self, future: bool = False):
        """ Explain the behaviour of the ego considering the last tau time-steps and the future predicted actions.

        Args:
            future: Whether to generate an explanation considering the future predict actions of vehicles.
        """
        current_t = len(self.observations[self.agent_id][0].states) - 1
        if self.tau > current_t:
            logger.warning(f"Cannot roll back observation by tau "
                           f"without enough time steps. ({self.tau} > {current_t}")
            return None

        # If no dataset was generated before or we are calling the function at a different timestep
        #  as the last dataset then generate a new dataset.
        if self.__dataset is None or \
                current_t != len(self.__previous_observations[self.agent_id][0].states) - 1 + self.tau:
            self.__generate_counterfactuals()

        assert self.__dataset is not None, f"No counterfactual data set present."

        # Generate final explanation
        query_present = {}
        query_not_present = {}
        for mid, fs in self.dataset.items():
            if fs.query_present:
                query_present[mid] = fs
            else:
                query_not_present[mid] = fs
        diffs = {}
        for component in self._reward.COMPONENTS:
            r_qp = [it.reward[component] for it in query_present.values()
                    if it.reward[component] is not None]
            r_qp = np.sum(r_qp) / len(r_qp) if r_qp else np.nan
            r_qnp = [it.reward[component] for it in query_not_present.values()
                     if it.reward[component] is not None]
            r_qnp = np.sum(r_qnp) / len(r_qnp) if r_qnp else np.nan
            diffs[component] = r_qp - r_qnp  # TODO: Consider using the weighted reward factors here to represent the actual decision process of the ego vehicle
        c_star = max(diffs, key=lambda k: np.abs(diffs[k]))
        r_star = diffs[c_star]

        xs, ys = [], []
        for mid, item in self.dataset.items():
            trajectories = {aid: traj.slice(0, current_t) if not future else traj.slice(current_t, None)
                            for aid, traj in item.trajectories.items()}
            xs.append(self.__features.to_features(self.agent_id, trajectories))
            ys.append(item.query_present)
        X, y = self.__features.binarise(xs, ys)
        model = LogisticRegression().fit(X, y)
        coeffs = model.coef_
        logger.info(coeffs)

        # TODO: Convert to NL explanations through language templates.

    def __generate_counterfactuals(self):
        """ Get observations from tau time steps before, and call MCTS from that joint state."""
        logger.info("Generating counterfactual rollouts.")
        previous_frame = {}
        self.__previous_observations = {}
        for agent_id, observation in self.observations.items():
            frame = observation[1]
            len_states = len(observation[0].states)
            if len_states > self.__tau:
                self.__previous_observations[agent_id] = (observation[0].slice(0, len_states - self.__tau), frame)
                previous_frame[agent_id] = observation[0].states[len_states - self.__tau - 1]

        if previous_frame:
            previous_observation = ip.Observation(previous_frame, self.__scenario_map)
            previous_goals = self.get_goals(previous_observation)
            self.__generate_rollouts(previous_observation, previous_goals)
            self.__dataset = self.__get_dataset()

    def __generate_rollouts(self, previous_observation: ip.Observation, previous_goals: List[ip.Goal]):
        """ Runs MCTS to generate a new sequence of macro actions to execute using previous observations.

        Args:
            previous_observation: Observation of the env tau time steps back.
            previous_goals: Possible goals from the state tau time steps back.
        """
        frame = previous_observation.frame
        agents_metadata = {aid: state.metadata for aid, state in frame.items()}
        self.__previous_goal_probabilities = {aid: ip.GoalsProbabilities(previous_goals)
                                              for aid in frame.keys() if aid != self.agent_id}
        visible_region = ip.Circle(frame[self.agent_id].position, self.view_radius)

        # Increase number of trajectories to generate
        n_trajectories = self._goal_recognition._n_trajectories
        self._goal_recognition._n_trajectories = self.__n_trajectories

        for agent_id in frame:
            if agent_id == self.agent_id:
                continue

            # Generate all possible trajectories for non-egos from tau time steps back
            gps = self.__previous_goal_probabilities[agent_id]
            self._goal_recognition.update_goals_probabilities(
                goals_probabilities=gps,
                observed_trajectory=self.__previous_observations[agent_id][0],
                agent_id=agent_id,
                frame_ini=self.__previous_observations[agent_id][1],
                frame=frame,
                visible_region=visible_region)

            # Set the probabilities equal for each goal and trajectory
            #  to make sure we can sample all counterfactual scenarios
            n_reachable = sum(map(lambda x: len(x) > 0, gps.trajectories_probabilities.values()))
            for goal, traj_prob in gps.trajectories_probabilities.items():
                traj_len = len(traj_prob)
                if traj_len > 0:
                    gps.goals_probabilities[goal] = 1 / n_reachable
                    gps.trajectories_probabilities[goal] = [1 / traj_len for _ in range(traj_len)]

        # Reset the number of trajectories for goal generation
        self._goal_recognition._n_trajectories = n_trajectories

        # Run MCTS search for counterfactual simulations while storing run results
        self.__previous_mcts.search(
            agent_id=self.agent_id,
            goal=self.goal,
            frame=frame,
            meta=agents_metadata,
            predictions=self.__previous_goal_probabilities)

    def __get_dataset(self) -> Dict[int, Item]:
        """ Return dataset recording states, boolean feature, and reward """
        dataset = {}
        mcts_results = self.__previous_mcts.results

        for m, rollout in enumerate(mcts_results):
            trajectories = {}
            r = []
            last_node = rollout.tree[rollout.trace[:-1]]
            sim_trajectory_ego = None

            # save trajectories of each agent
            for agent_id, agent in last_node.run_results[-1].agents.items():
                trajectory = ip.StateTrajectory(self.fps)
                trajectory.extend(self.__previous_observations[agent_id][0], reload_path=False)
                sim_trajectory = agent.trajectory_cl.slice(1, None)

                # Retrieve maneuvers and macro actions for non-ego vehicles
                if agent_id != self.agent_id:
                    plan = self.tau_goals_probabilities[agent_id].trajectory_to_plan(*rollout.samples[agent_id])
                    fill_missing_actions(sim_trajectory, plan)
                else:
                    sim_trajectory_ego = sim_trajectory

                trajectory.extend(sim_trajectory, reload_path=True)
                trajectories[agent_id] = trajectory

            # save reward for each component
            for last_action, reward_value, in last_node.reward_results.items():
                if last_action == rollout.trace[-1]:
                    r = reward_value[-1].reward_components

            data_set_m = Item(trajectories, self.get_outcome_y(sim_trajectory_ego), r)
            dataset[m] = data_set_m

        logger.info('Counterfactual dataset generation done.')
        return dataset

    @property
    def dataset(self) -> Dict[int, Item]:
        """ The generated set of counterfactual features with the key being its index. """
        return self.__dataset

    @property
    def tau_goals_probabilities(self) -> Dict[int, ip.GoalsProbabilities]:
        """ The goal and trajectory probabilities inferred from tau time steps ago. """
        return self.__previous_goal_probabilities

    def get_outcome_y(self, trajectory: ip.StateTrajectory) -> bool:
        """ Return boolean value for each predefined feature

        Args:
            trajectory: trajectory of ego
        """
        maneuver = self.__user_query["maneuver"]
        return self.__matching.maneuver_matching(maneuver, trajectory)
        # return np.any(trajectories[0].velocity < 0.1)  # S2

    @property
    def tau(self) -> int:
        """ Parameter for how many time steps to rollback for counterfactual generation. """
        return self.__tau
