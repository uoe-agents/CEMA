import numpy as np
import logging

import igp2 as ip
from typing import Dict, List
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class Features:
    """ Class to store a (counter)factual rollout generated by XAVIAgent. """
    states: Dict[int, ip.StateTrajectory]
    feature_exist: Dict[int, List[bool]]
    reward: Dict[str, float]


class XAVIAgent(ip.MCTSAgent):
    """ Generate new rollouts and save results after MCTS in the observation tau time before. """

    def __init__(self,
                 agent_id: int,
                 initial_state: ip.AgentState,
                 t_update: float,
                 scenario_map: ip.Map,
                 goal: ip.Goal = None,
                 view_radius: float = 50.0,
                 fps: int = 20,
                 kinematic: bool = False,
                 n_simulations: int = 5,
                 max_depth: int = 3,
                 store_results: str = 'final',
                 cost_factors: Dict[str, float] = None,
                 reward_factors: Dict[str, float] = None,
                 velocity_smoother_params: dict = None,
                 goal_recognition_params: dict = None):

        super(XAVIAgent, self).__init__(agent_id, initial_state, t_update, scenario_map, goal, view_radius,
                                        fps, kinematic, n_simulations, max_depth, store_results,
                                        cost_factors, reward_factors, velocity_smoother_params,
                                        goal_recognition_params)

        self._previous_goal_probabilities = None
        self._previous_mcts = ip.MCTS(scenario_map, n_simulations=n_simulations, max_depth=max_depth,
                                      store_results=store_results)

        self._scenario_map = scenario_map
        self._tau = fps
        self._previous_observations = {}
        self._dataset = {}

    def rollout_generation(self):
        """ get observations tau time before, and call MCTS."""
        previous_state = {}
        for agent_id, observation in self.observations.items():
            frame = observation[1]
            len_states = len(observation[0].states)
            if len_states > self._tau:
                self._previous_observations[agent_id] = (observation[0].slice(0, len_states - self._tau), frame)
                previous_state[agent_id] = observation[0].states[len_states - self._tau - 1]

        if previous_state:
            previous_observation = ip.Observation(previous_state, self._scenario_map)
            self.get_goals(previous_observation)
            self.update_previous_plan(previous_observation)
            self._dataset = self.get_dataset(previous_observation)

    def update_previous_plan(self, previous_observation: ip.Observation):
        """ Runs MCTS to generate a new sequence of macro actions to execute using previous observations."""
        frame = previous_observation.frame
        agents_metadata = {aid: state.metadata for aid, state in frame.items()}
        self._previous_goal_probabilities = {aid: ip.GoalsProbabilities(self._goals)
                                             for aid in frame.keys() if aid != self.agent_id}
        visible_region = ip.Circle(frame[self.agent_id].position, self.view_radius)

        for agent_id in frame:
            if agent_id == self.agent_id:
                continue

            self._goal_recognition.update_goals_probabilities(self._previous_goal_probabilities[agent_id],
                                                              self._previous_observations[agent_id][0],
                                                              agent_id,
                                                              self._previous_observations[agent_id][1],
                                                              frame,
                                                              visible_region=visible_region)
        # set the probabilities equal for each goal
        for goal_id, goals_prob in self._previous_goal_probabilities.items():
            for traj_id, traj_prob in goals_prob.trajectories_probabilities.items():
                traj_len = len(traj_prob)
                if traj_len > 1:
                    self._previous_goal_probabilities[goal_id].trajectories_probabilities[traj_id] = [1/traj_len for i in range(traj_len)]

        self._previous_mcts.search(self.agent_id, self.goal, frame,
                                   agents_metadata, self._previous_goal_probabilities)

    @staticmethod
    def get_outcome_y(states: Dict[int, ip.AgentState]) -> Dict[int, List[bool]]:
        """ Return boolean value for each predefined feature """
        """
        Args:
            states: the state of all agents.
        """
        features = {
            'accelerating': False,
            'decelerating': False,
            'maintaining': False,
            'relative slower': False,
            'relative faster': False,
            'same speed': False,
            'ever stop': False,
            'mu': False,
            'omega': False,
            'safety': False
        }
        y = {}
        for agent_id, state in states.items():
            if agent_id == 0:
                acc = np.mean(state.acceleration)
                if acc > 0:
                    features['accelerating'] = True
                elif acc == 0:
                    features['maintaining'] = True
                else:
                    features['decelerating'] = True
            y[agent_id] = list(features.values())
        return y

    def get_dataset(self, previous_observation: ip.Observation) -> Dict[int, Features]:
        """ Return dataset recording states, boolean feature, and reward """
        dataset = {}
        mcts_results = self._previous_mcts.results
        if isinstance(mcts_results, ip.MCTSResult):
            mcts_results = ip.AllMCTSResult()
            mcts_results.add_data(self._previous_mcts.results)

        for m, rollout in enumerate(mcts_results):
            states = {}
            r = []
            last_node = list(rollout.tree.tree)[-1]
            for node, node_value in rollout.tree.tree.items():
                if node == last_node:
                    # save trajectories of each agent
                    for agent_id, agent in node_value.run_results[0].agents.items():
                        states[agent_id] = agent.trajectory_cl
                    # save reward for each component
                    if node == last_node:
                        for node_reward, reward_value, in node_value.reward_results.items():
                            r = reward_value[0].reward_components

            data_set_m = Features(states, self.get_outcome_y(states), r)
            dataset[m] = data_set_m
        logger.info('dataset is saved')

        return dataset
