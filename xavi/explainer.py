import pickle

import numpy as np
import pandas as pd
import logging

import igp2 as ip
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from sklearn.linear_model import LogisticRegression

from xavi.features import Features
from xavi.util import fill_missing_actions, truncate_observations, \
    to_state_trajectory, Observations
from xavi.matching import ActionMatching, ActionGroup
from xavi.query import Query, QueryType

logger = logging.getLogger(__name__)


@dataclass
class Item:
    """ Class to store a (counter)factual trajectories, the actions of the ego,
    the rewards, and the query observation generated by MCTS. """
    trajectories: Dict[int, ip.StateTrajectory]
    action_present: bool
    reward: Dict[str, float]


class XAVIAgent(ip.MCTSAgent):
    """ Generate new rollouts and save results after MCTS in the observation tau time before. """

    def __init__(self,
                 cf_n_trajectories: int = 3,
                 cf_n_simulations: int = 15,
                 cf_max_depth: int = 5,
                 **kwargs):
        """ Create a new XAVIAgent.

        Args:
            tau: The interval to roll back for counterfactual generation. By default set to FPS.
            cf_n_trajectories: Number of maximum trajectories to generate with A*.
            cf_n_simulations: Number of MCTS simulations to run for counterfactual generation.
            cf_d_max: Maximum MCTS search depth for counterfactual simulations.

        Keyword Args: See arguments of parent-class MCTSAgent.
        """

        super(XAVIAgent, self).__init__(**kwargs)

        self.__n_trajectories = cf_n_trajectories
        self.__previous_goal_probabilities = None
        self.__previous_mcts = ip.MCTS(scenario_map=kwargs["scenario_map"],
                                       n_simulations=kwargs.get("cf_n_simulations", cf_n_simulations),
                                       max_depth=kwargs.get("cf_max_depth", cf_max_depth),
                                       store_results="all")
        self.__features = Features()
        self.__matching = ActionMatching()
        self.__scenario_map = kwargs["scenario_map"]

        self.__previous_observations = None
        self.__previous_dataset = None
        self.__t_dataset = None
        self.__user_query = None
        self.__current_t = None

    def explain_actions(self, user_query: Query) -> Any:  # TODO: Replace return value once we know what it is.
        """ Explain the behaviour of the ego considering the last tau time-steps and the future predicted actions.

        Args:
            user_query: The parsed query of the user.
        """
        self.__user_query = user_query
        self.__current_t = self.observations[self.agent_id][0].states[-1].time

        # Determine timing information of the query.
        self.query.get_tau(self.observations)

        if self.query.type == QueryType.WHAT:
            return self.__explain_what()

        # Generate new or update existing dataset.
        if self.__previous_dataset is None or \
                self.__current_t != self.__t_dataset:
            self.__get_counterfactuals()
            self.__t_dataset = self.__current_t

        assert self.__previous_dataset is not None, f"No counterfactual data set present."

        final_causes = self.__final_causes()

        if self.query.type == QueryType.WHAT_IF:
            # TODO: Determine most likely action under counterfactual condition.
            pass

        past_causes, future_causes = self.__efficient_causes()

        # TODO: Convert to NL explanations through language templates.

    def __explain_what(self) -> ActionGroup:
        """ Generate an explanation to a what query. Involves looking up the trajectory segment at T and
        returning a feature set of it. We assume for the future that non-egos follow their MAP-prediction for
        goal and trajectory.

        Returns:
            A action group of the executed action at the user given time point.
        """
        if self.query.agent_id is None:
            logger.warning(f"No Agent ID given for what-query. Falling back to ego ID.")
            self.query.agent_id = self.agent_id

        # Use observations until current time
        trajectory = ip.StateTrajectory(self.fps)
        trajectory.extend(self.observations[self.query.agent_id][0], reload_path=False)

        map_preds = {aid: p.map_prediction() for aid, p in self.goal_probabilities.items()}
        if self.query.agent_id == self.agent_id:
            optimal_rollouts = self.mcts.results.optimal_rollouts
            matching_rollout = None
            for rollout in optimal_rollouts:
                for aid, map_pred in map_preds.items():
                    if rollout.samples[aid] != map_pred: break
                else:
                    matching_rollout = rollout
                    break
            last_node = matching_rollout.tree[matching_rollout.trace[:-1]]
            agent = last_node.run_results[-1].agents[self.query.agent_id]
            sim_trajectory = agent.trajectory_cl
        else:
            goal, sim_trajectory = map_preds[self.query.agent_id]
            plan = self.goal_probabilities[self.query.agent_id].trajectory_to_plan(goal, sim_trajectory)
            sim_trajectory = to_state_trajectory(sim_trajectory, plan, self.fps)

        # Truncate trajectory to time step nearest to the final observation of the agent
        last_point = self.observations[self.query.agent_id][0].path[-1]
        closest_idx = np.argmin(np.linalg.norm(sim_trajectory.path - last_point, axis=1))
        sim_trajectory = sim_trajectory.slice(closest_idx, None)
        trajectory.extend(sim_trajectory, reload_path=True)

        start_t = self.query.t_action
        if start_t >= len(trajectory):
            logger.warning(f"Total trajectory for Agent {self.query.agent_id} is not "
                           f"long enough for query! Falling back to final timestep.")
            start_t = len(trajectory) - 1
        segments = self.__matching.action_segmentation(trajectory)
        grouped_segments = ActionGroup.group_by_maneuver(segments)
        return [seg for seg in grouped_segments if seg.start <= start_t <= seg.end][0]

    def __final_causes(self) -> pd.DataFrame:
        """ Generate final causes for the queried action.

        Returns:
            Dataframe of reward components with the absolute and relative changes for each component.
        """
        query_present = {}
        query_not_present = {}
        for mid, item in self.cf_dataset.items():
            # TODO: Filter for trajectories that match the observations between tau and t_action_start.
            if item.action_present:
                query_present[mid] = item
            else:
                query_not_present[mid] = item

        diffs = {}
        for component in self._reward.COMPONENTS:
            factor = self._reward.factors[component]
            r_qp = [item.reward[component] for item in query_present.values()
                    if item.reward[component] is not None]
            r_qp = factor * np.sum(r_qp) / len(r_qp) if r_qp else np.nan
            r_qnp = [item.reward[component] for item in query_not_present.values()
                     if item.reward[component] is not None]
            r_qnp = factor * np.sum(r_qnp) / len(r_qnp) if r_qnp else np.nan
            abs_diff = r_qp - r_qnp
            diffs[component] = (abs_diff, abs_diff / r_qnp)
        df = pd.DataFrame.from_dict(diffs, orient="index", columns=["absolute", "relative"])
        return df.sort_values(ascending=False, by="absolute")

    def __efficient_causes(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ Generate efficient causes for the queried action.

        Returns:
            Dataframes for past and future causes with causal effect size.
        """
        xs_past, ys_past = [], []
        xs_future, ys_future = [], []
        for m, item in self.cf_dataset.items():
            trajectories_past, trajectories_future = {}, {}
            for aid, traj in item.trajectories.items():
                trajectories_past[aid] = traj.slice(0, self.query.t_action)
                trajectories_future[aid] = traj.slice(self.query.t_action, None)
            xs_past.append(self.__features.to_features(self.agent_id, trajectories_past))
            xs_future.append(self.__features.to_features(self.agent_id, trajectories_future))
            ys_past.append(item.action_present)
            ys_future.append(item.action_present)
        X_past, y_past = self.__features.binarise(xs_past, ys_past)
        X_future, y_future = self.__features.binarise(xs_future, ys_future)
        model_past = LogisticRegression().fit(X_past, y_past)
        model_future = LogisticRegression().fit(X_future, y_future)

        coefs_past = pd.DataFrame(
            np.squeeze(model_past.coef_) * X_past.std(axis=0),
            columns=["Coefficient importance"],
            index=X_past.columns,
        )
        coefs_future = pd.DataFrame(
            np.squeeze(model_future.coef_) * X_future.std(axis=0),
            columns=["Coefficient importance"],
            index=X_future.columns,
        )
        return coefs_past, coefs_future

    # ---------Counterfactual rollout generation---------------

    def __get_counterfactuals(self):
        """ Get observations from tau time steps before, and call MCTS from that joint state."""
        logger.info("Generating counterfactual rollouts.")

        self.__previous_observations, previous_frame = \
            truncate_observations(self.observations, self.query.tau)

        if previous_frame:
            previous_observation = ip.Observation(previous_frame, self.__scenario_map)
            previous_goals = self.get_goals(previous_observation)
            self.__generate_rollouts(previous_observation, previous_goals)
            self.__previous_dataset = self.__get_dataset(
                self.__previous_mcts.results, self.__previous_observations)

    def __generate_rollouts(self,
                            previous_observation: ip.Observation,
                            previous_goals: List[ip.Goal]):
        """ Runs MCTS to generate a new sequence of macro actions to execute using previous observations.

        Args:
            previous_observation: Observation of the env tau time steps back.
            previous_goals: Possible goals from the state tau time steps back.
        """
        frame = previous_observation.frame
        agents_metadata = {aid: state.metadata for aid, state in frame.items()}
        self.__previous_goal_probabilities = \
            {aid: ip.GoalsProbabilities(previous_goals)
             for aid in frame.keys() if aid != self.agent_id}
        visible_region = ip.Circle(frame[self.agent_id].position, self.view_radius)

        # Increase number of trajectories to generate
        n_trajectories = self._goal_recognition._n_trajectories
        self._goal_recognition._n_trajectories = self.__n_trajectories

        for agent_id in frame:
            if agent_id == self.agent_id:
                continue

            # Generate all possible trajectories for non-egos from tau time steps back
            gps = self.__previous_goal_probabilities[agent_id]
            self._goal_recognition.update_goals_probabilities(
                goals_probabilities=gps,
                observed_trajectory=self.__previous_observations[agent_id][0],
                agent_id=agent_id,
                frame_ini=self.__previous_observations[agent_id][1],
                frame=frame,
                visible_region=visible_region)

            # Set the probabilities equal for each goal and trajectory
            #  to make sure we can sample all counterfactual scenarios
            n_reachable = sum(map(lambda x: len(x) > 0, gps.trajectories_probabilities.values()))
            for goal, traj_prob in gps.trajectories_probabilities.items():
                traj_len = len(traj_prob)
                if traj_len > 0:
                    gps.goals_probabilities[goal] = 1 / n_reachable
                    gps.trajectories_probabilities[goal] = [1 / traj_len for _ in range(traj_len)]

        # Reset the number of trajectories for goal generation
        self._goal_recognition._n_trajectories = n_trajectories

        # Run MCTS search for counterfactual simulations while storing run results
        self.__previous_mcts.search(
            agent_id=self.agent_id,
            goal=self.goal,
            frame=frame,
            meta=agents_metadata,
            predictions=self.__previous_goal_probabilities)

    def __get_dataset(self,
                      mcts_results: ip.AllMCTSResult,
                      observations: Observations) \
            -> Dict[int, Item]:
        """ Return dataset recording states, boolean feature, and reward

         Args:
             mcts_results: MCTS results class to convert to a dataset.
             observations: The observations that preceded the planning step.
         """
        dataset = {}

        for m, rollout in enumerate(mcts_results):
            trajectories = {}
            r = []
            last_node = rollout.tree[rollout.trace[:-1]]
            trajectory_queried_agent = None

            # save trajectories of each agent
            for agent_id, agent in last_node.run_results[-1].agents.items():
                trajectory = ip.StateTrajectory(self.fps)
                trajectory.extend(observations[agent_id][0], reload_path=False)
                sim_trajectory = agent.trajectory_cl.slice(1, None)

                # Retrieve maneuvers and macro actions for non-ego vehicles
                if agent_id != self.agent_id:
                    plan = self.cf_goals_probabilities[agent_id].trajectory_to_plan(*rollout.samples[agent_id])
                    fill_missing_actions(sim_trajectory, plan)

                if (self.query.type in ["why", "whynot"] and agent_id == self.agent_id) or \
                        (self.query.type == "whatif" and agent_id == self.query.agent_id):
                    trajectory_queried_agent = sim_trajectory

                trajectory.extend(sim_trajectory, reload_path=True)
                trajectories[agent_id] = trajectory

            # save reward for each component
            for last_action, reward_value, in last_node.reward_results.items():
                if last_action == rollout.trace[-1]:
                    r = reward_value[-1].reward_components

            # Determine outcome
            y = self.__matching.action_matching(self.query.action, trajectory_queried_agent)

            data_set_m = Item(trajectories, y, r)
            dataset[m] = data_set_m

        logger.debug('Dataset generation done.')
        return dataset

    # -------------Field access properties-------------------

    @property
    def cf_dataset(self) -> Dict[int, Item]:
        """ The most recently generated set of counterfactual features with the key being its index. """
        return self.__previous_dataset

    @property
    def query(self) -> Query:
        """ The most recently asked user query. """
        return self.__user_query

    @property
    def cf_goals_probabilities(self) -> Dict[int, ip.GoalsProbabilities]:
        """ The goal and trajectory probabilities inferred from tau time steps ago. """
        return self.__previous_goal_probabilities
